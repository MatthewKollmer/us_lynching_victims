{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2d21607e724804",
   "metadata": {},
   "source": [
    "# Collaborative Data Labelling\n",
    "\n",
    "## Overview\n",
    "\n",
    "At this step, I needed to label my random sample so it could be used to fine-tune BERT to classify the rest of my data.\n",
    "\n",
    "I was fortunate to have help in this regard since labelling data can take some time. To divvy up the task efficiently, I took the ten chunks of my training data–100 newspaper clippings apiece–and uploaded them to Github. Then I created 10 nearly identical Google Colab notebooks, one for each chunk. The only differences between these notebooks were the .csv files they reviewed and the filenames they saved.\n",
    "\n",
    "Here's the code and text as it appeared in these notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b5be3a526c54233",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "# from google.colab import files\n",
    "import textwrap"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b204a8bc7ac3ad81",
   "metadata": {},
   "source": [
    "## 1) Directions\n",
    "\n",
    "When you run the code in the following notebook, you will be prompted to label newspaper clippings. The goal is to identify if the clippings contain any reference to lynchings. The labelled data will then be used to fine-tune a BERT model to classify other clippings.\n",
    "\n",
    "Please label the data in the following way:\n",
    "\n",
    "- '__yes__' = yes, the clipping is contains reference to a lynching\n",
    "- '__no__' = no, the clipping has nothing to do with lynchings\n",
    "\n",
    "Once you've finished labelling the data, run the last bit of code to save your work as a .csv file. Then please email this .csv file to Matthew Kollmer at kollmer2@illinois.edu.\n",
    "\n",
    "Thank you for your help!"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7cec1e4728d0b7e",
   "metadata": {},
   "source": [
    "# df was linked to different .csv files, one per chunk (i.e. part_1, part_2, part_3, etc.)\n",
    "df = pd.read_csv('training_data/clippings_part_10.csv')\n",
    "\n",
    "case_match_values = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for index, clipping in df['clippings'].items():\n",
    "    counter += 1\n",
    "    print('---------------------------------------')\n",
    "    print('---------------------------------------')\n",
    "    print('---------------------------------------')\n",
    "    print(f'row: {counter}')\n",
    "    wrapped_clipping = '\\n'.join(textwrap.wrap(str(clipping), width=60))\n",
    "    print(wrapped_clipping)\n",
    "    print()\n",
    "    case_match = input(f'Does the clipping contain text about a lynching? Answers must be yes or no: ')\n",
    "    case_match_values.append(case_match)\n",
    "\n",
    "df['case_match'] = case_match_values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "330f3ab04c29ba79",
   "metadata": {},
   "source": [
    "## 2) Save and Finish\n",
    "\n",
    "Once you've finished labelling the set of 100 clippings, run the last bit of code below. It will save the labelled data as a .csv file.\n",
    "\n",
    "Last thing: please email this file to Matthew. Thanks again!"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d733513342c1dc8",
   "metadata": {},
   "source": [
    "# changed depending on which chunk was labelled (i.e. part_1, part_2, part_3, etc.)\n",
    "df.to_csv('training_data/clippings_part_10_labelled.csv', index=False)\n",
    "# files.download('clippings_part_1_labelled.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# BREAK - Collaborative Work Ended Here\n",
    "\n",
    "## 3) Unification of Data\n",
    "\n",
    "Once all twenty subsets of the data were labelled, I concatenated them into one dataframe and saved it as training_data.csv."
   ],
   "id": "22c541341e9fb91e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df1 = pd.read_csv('training_data/clippings_part_1_labelled.csv')\n",
    "df2 = pd.read_csv('training_data/clippings_part_2_labelled.csv')\n",
    "df3 = pd.read_csv('training_data/clippings_part_3_labelled.csv')\n",
    "df4 = pd.read_csv('training_data/clippings_part_4_labelled.csv')\n",
    "df5 = pd.read_csv('training_data/clippings_part_5_labelled.csv')\n",
    "df6 = pd.read_csv('training_data/clippings_part_6_labelled.csv')\n",
    "df7 = pd.read_csv('training_data/clippings_part_7_labelled.csv')\n",
    "df8 = pd.read_csv('training_data/clippings_part_8_labelled.csv')\n",
    "df9 = pd.read_csv('training_data/clippings_part_9_labelled.csv')\n",
    "df10 = pd.read_csv('training_data/clippings_part_10_labelled.csv')\n",
    "df11 = pd.read_csv('training_data/clippings_part_11_labelled.csv')\n",
    "df12 = pd.read_csv('training_data/clippings_part_12_labelled.csv')\n",
    "df13 = pd.read_csv('training_data/clippings_part_13_labelled.csv')\n",
    "df14 = pd.read_csv('training_data/clippings_part_14_labelled.csv')\n",
    "df15 = pd.read_csv('training_data/clippings_part_15_labelled.csv')\n",
    "df16 = pd.read_csv('training_data/clippings_part_16_labelled.csv')\n",
    "df17 = pd.read_csv('training_data/clippings_part_17_labelled.csv')\n",
    "\n",
    "train_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17], ignore_index=True)"
   ],
   "id": "a5a683695e915106",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I only added the 'yes' labels of the last three subsets:",
   "id": "45b664020b5c8567"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df18 = pd.read_csv('training_data/clippings_part_18_labelled.csv')\n",
    "df19 = pd.read_csv('training_data/clippings_part_19_labelled.csv')\n",
    "df20 = pd.read_csv('training_data/clippings_part_20_labelled.csv')\n",
    "\n",
    "df18_yes = df18[df18['case_match'] == 'yes']\n",
    "df19_yes = df19[df19['case_match'] == 'yes']\n",
    "df20_yes = df20[df20['case_match'] == 'yes']\n",
    "\n",
    "train_df = pd.concat([train_df, df18_yes, df19_yes, df20_yes], ignore_index=True)"
   ],
   "id": "1a267a5fee955206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, I saved train_df:",
   "id": "8fc1cac53cd8b030"
  },
  {
   "cell_type": "code",
   "id": "ec64a1ee8321e227",
   "metadata": {},
   "source": "train_df.to_csv('training_data/training_data.csv', index=False)",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
