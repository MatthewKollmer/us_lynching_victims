{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Enriching Data with Location Information\n",
    "\n",
    "### 1) Overview\n",
    "\n",
    "The following notebook shows my processes for adding lynching locations, newspaper locations, and other newspaper metadata to our dataset. This notebook relies on our initial data sources (the Tolnay-Beck Inventory and the Seguin-Rigby Dataset) as well as newspaper location data compiled by Viral Texts Project. I've also used metadata encoded in the Chronicling America URLs to build this pipeline.\n",
    "\n",
    "The final result from the steps below (and the steps taken in previous notebooks) are that all the csv files (one per victim) contain the following data:\n",
    "\n",
    "- victim (victim's name)\n",
    "- race (Black or white)\n",
    "- gender (if known)\n",
    "- lynch_date (specified to either the day or the month)\n",
    "- lynch_location (town/city, county/parish, and state, if known)\n",
    "- lynch_latitude (based on lynch_location)\n",
    "- lynch_longitude (based on lynch_location)\n",
    "- newspaper (title of newspaper)\n",
    "- reprint_date (date of specified page)\n",
    "- reprint_longitude (based on newspaper location)\n",
    "- reprint_latitude (based on newspaper location)\n",
    "- clippings (50 words before victim name and the 100 words after victim name)\n",
    "- text (the full OCR text of the given page)\n",
    "- probability (our likelihood that the clipping contains reference to a lynching, labelled as either 'high', 'medium', 'low', 'unlikely', or 'unknown')\n",
    "- BERT_1 (first round of BERT classification, either 'yes' or 'no')\n",
    "- BERT_2 (second round of BERT classification, either 'yes' or 'no')\n",
    "- BERT_3 (third round of BERT classification, either 'yes' or 'no')\n",
    "- violence_word_count (the number of times words from our violence word lexicon appear in the clipping)\n",
    "- racist_word_count (the number of times words from our racist word lexicon appear in the clipping)\n",
    "- page_details (newspaper title, city, state, issue, and page number taken from our Chron Am search results)\n",
    "- url (the Chron Am URL to the given page)\n",
    "- sn_code (Chron Am's newspaper code for the given newspaper)\n",
    "- coverage (the dbpedia url used to cross-reference newspaper locations)"
   ],
   "id": "c154e697b9222822"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ],
   "id": "93f7c44c63991a88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2) Getting Lynch Location Data\n",
    "\n",
    "First I revisited the unified lynch directories created with 01_unify_data_sources.ipynb (our combined version of the Tolnay-Beck and Seguin-Rigby datasets). By cross-referencing victim names in this dataset with the victim names in the csv files, I was able to pull the lynch location (city/town, county, state) and the latitude and longitude."
   ],
   "id": "8a7de4935e1e2030"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lynch_inventory = pd.read_csv('subset_cleaned_combined_lynch_inventories.csv')\n",
    "lynch_inventory.head()"
   ],
   "id": "74b963b433bc09b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I started by creating a dictionary of location data from lynch_inventory for cross-referencing. I also defined the directory and csv file paths.",
   "id": "97d1679f1f14c310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "location_data_dictionary = {}\n",
    "for _, row in lynch_inventory.iterrows():\n",
    "    victim_name = row['victim_name']\n",
    "    location = row['lynch_location']\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "    location_data_dictionary[victim_name] = (location, latitude, longitude)\n",
    "\n",
    "directory = 'name_clusters'\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]"
   ],
   "id": "7fb00dfe020f6bb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then I used the following loop to add the dictionary location data if/when the victim name in the given csv file matched a victim name in the dictionary. I added the progress bar to keep track of processing time, as usual.",
   "id": "2ac1acb1b2f0b716"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_rows = 0\n",
    "all_csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "for filename in all_csv_files:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    total_rows += len(df)\n",
    "\n",
    "pbar = tqdm(total=total_rows, desc='rows')\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df_temp = pd.read_csv(file_path)\n",
    "    added_locations = []\n",
    "    added_latitudes = []\n",
    "    added_longitudes = []\n",
    "    \n",
    "    for victim_name in df_temp['victim']:\n",
    "        if victim_name in location_data_dictionary:\n",
    "            location, latitude, longitude = location_data_dictionary[victim_name]\n",
    "        else:\n",
    "            location, latitude, longitude = (None, None, None)\n",
    "        \n",
    "        added_locations.append(location)\n",
    "        added_latitudes.append(latitude)\n",
    "        added_longitudes.append(longitude)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    df_temp['lynch_location'] = added_locations\n",
    "    df_temp['lynch_latitude'] = added_latitudes\n",
    "    df_temp['lynch_longitude'] = added_longitudes\n",
    "    df_temp.to_csv(file_path, index=False)\n",
    "\n",
    "pbar.close()"
   ],
   "id": "879267cfe684c81f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3) Getting SN Code and Reprint Date\n",
    "\n",
    "At this step, I used data encoded in the Chron Am URLs. Basically, I just referenced the url patterns using regular expressions for the Chron Am sn codes and reprint dates. For more info about these patterns, check out this page on the Chron Am API: [https://chroniclingamerica.loc.gov/about/api/](https://chroniclingamerica.loc.gov/about/api/)."
   ],
   "id": "c66f72cf61f9d4b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# here's a regular expression for capturing the sn codes in the Chron Am url\n",
    "sn_code_regex = re.compile(r'sn\\d{8}')\n",
    "\n",
    "total_rows = 0\n",
    "all_csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "for filename in all_csv_files:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    total_rows += len(df)\n",
    "\n",
    "pbar = tqdm(total=total_rows, desc='rows')\n",
    "\n",
    "for filename in all_csv_files:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    sn_codes = []\n",
    "    for url in df['url']:\n",
    "        if pd.isna(url):\n",
    "            sn_codes.append(None)\n",
    "        else:\n",
    "            sn_match = sn_code_regex.search(url)\n",
    "            if sn_match:\n",
    "                sn_codes.append(sn_match.group(0))\n",
    "            else:\n",
    "                sn_codes.append(None)\n",
    "        \n",
    "        pbar.update(1)\n",
    "    \n",
    "    df['sn_code'] = sn_codes\n",
    "    df['sn_code'] = df['sn_code'].apply(lambda x: '/lccn/' + x if x else None)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "pbar.close()"
   ],
   "id": "16f8c2566d7480dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# here's a regex pattern for capturing dates in Chron Am's URLs\n",
    "date_regex = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "\n",
    "total_rows = 0\n",
    "all_csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "for filename in all_csv_files:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    total_rows += len(df)\n",
    "\n",
    "pbar = tqdm(total=total_rows, desc='rows')\n",
    "\n",
    "for filename in all_csv_files:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    reprint_dates = []\n",
    "    for url in df['url']:\n",
    "        if pd.isna(url):\n",
    "            reprint_dates.append(None)\n",
    "        else:\n",
    "            date_match = date_regex.search(url)\n",
    "            if date_match:\n",
    "                reprint_dates.append(date_match.group(0))\n",
    "            else:\n",
    "                reprint_dates.append(None)\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "    df['reprint_date'] = reprint_dates\n",
    "    df['reprint_date'] = pd.to_datetime(df['reprint_date'], format='%Y-%m-%d', errors='coerce')   \n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "pbar.close()"
   ],
   "id": "1b15d9c7f4320d69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4) Getting Newspaper Title\n",
    "\n",
    "At this step, I first had to change the 'newspaper' column name to 'page_details'. If you remember from our scraping processes in steps 02 and 03, we pulled the 'newspaper' data from the Chron Am search results. This data included newspaper name, city/state, issue, and page number all in one column. So, to make this column more clear, I first renamed it to 'page_details'. Then I used a regular expression to pull the newspaper titles from page_details and save them in the new column called 'newspaper'."
   ],
   "id": "96e8f09e4b6b724f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# loop that simply changes column names (newspaper becomes page_details)\n",
    "for filename in all_csv_files:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.rename(columns={'newspaper': 'page_details'}, inplace=True)\n",
    "    df.to_csv(file_path, index=False)"
   ],
   "id": "46fe3a51f8c2be1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# here's the regex pattern I used to extract newspaper name from page_details\n",
    "# FYI: the newspaper name always comes first in page_details. It is either followed by '. [' or '. ('\n",
    "newspaper_pattern = re.compile(r'\\.\\ \\[|\\.\\(')\n",
    "\n",
    "# just a quick function that splits the string in the inputted column using the newspaper_pattern\n",
    "def extract_newspaper_with_regex(text):\n",
    "    paper_title = newspaper_pattern.split(str(text), maxsplit=1)\n",
    "    return paper_title[0]\n",
    "\n",
    "for filename in all_csv_files:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['newspaper'] = df['page_details'].apply(extract_newspaper_with_regex) # function applied\n",
    "    df.to_csv(file_path, index=False)"
   ],
   "id": "c92bc517c58a676f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5) Getting Newspaper Latitude & Longitude\n",
    "\n",
    "At this step, I'm relying on newspaper location data from the Viral Texts Project. This data has corresponding DBpedia links to newspapers. It also has latitude/longitude data to the corresponding the DBpedia links. This means I had to essentially cross-reference the three data–our newspapers by sn code to the DBpedia links, and then the DBpedia links to the lat/long data."
   ],
   "id": "20fe75a05291863e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lat_long_df = pd.read_csv('https://raw.githubusercontent.com/ViralTexts/newspaper-metadata/main/places.csv')\n",
    "series_df = pd.read_csv('https://raw.githubusercontent.com/ViralTexts/newspaper-metadata/main/series.csv')"
   ],
   "id": "644ee645d33eb3ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# I start by mapping the series coverage data (the DBpedia links) to a dictionary for faster reference to sn codes\n",
    "# then adding the coverage links into a new column \n",
    "coverage_dictionary = dict(zip(series_df['series'], series_df['coverage']))\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "        \n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['coverage'] = df['sn_code'].map(coverage_dictionary)\n",
    "    df.to_csv(file_path, index=False)"
   ],
   "id": "91e76f58637c5803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# then a dictionary with lat/long data\n",
    "# and adding lat/long data in new columns\n",
    "latitude_longitude_dictionary = dict(zip(lat_long_df['coverage'], zip(lat_long_df['lon'], lat_long_df['lat'])))\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['reprint_longitude'], df['reprint_latitude'] = zip(*df['coverage'].apply(lambda c: latitude_longitude_dictionary.get(c, (None, None))))\n",
    "    df.to_csv(file_path, index=False)"
   ],
   "id": "8f411e4c1f8c5ee7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6) Re-ordering Columns for Ease of Use\n",
    "\n",
    "At this point, I've got all the data in the csv files that I had planned to include. For ease of use, however, I thought it'd be better to reorganize the order of the columns. So, I created the following order:"
   ],
   "id": "5792b00c5ec65a47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "desired_order = ['victim', 'race', 'gender', 'lynch_date', 'lynch_location', 'lynch_latitude', 'lynch_longitude', 'newspaper', 'reprint_date', 'reprint_longitude', 'reprint_latitude', 'clippings', 'text', 'probability', 'BERT_1', 'BERT_2', 'BERT_3', 'violence_word_count', 'racist_word_count', 'page_details', 'url', 'sn_code', 'coverage']\n",
    "\n",
    "all_csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "for filename in all_csv_files:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    reordered_columns = [col for col in desired_order if col in df.columns]\n",
    "    df = df[reordered_columns]\n",
    "    df.to_csv(file_path, index=False)"
   ],
   "id": "8665dcb25076601e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7) Separating Victim Files by Race\n",
    "\n",
    "One last thing: I've been compiling data for both Black and white victims of lynchings so I can compare newspaper reports by race in later work. For VRT's purposes, though, I only want to provide data for Black victims. So, I've separated the files by race. "
   ],
   "id": "6181ee57180800f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# how I moved Black victim csvs into a new directory called 'black_victims'\n",
    "black_directory = os.path.join(directory, 'black_victims')\n",
    "os.makedirs(black_directory, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    file_path = os.path.join(directory, filename)\n",
    "    if file_path.startswith(black_directory):\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'race' in df.columns:\n",
    "        race_values = df['race'].astype(str)\n",
    "        if 'black' in race_values.unique():\n",
    "            dest_path = os.path.join(black_directory, filename)\n",
    "            shutil.move(file_path, dest_path)"
   ],
   "id": "75e58358ac62dff3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# how I moved white victim csvs into a new directory called 'white_victims'\n",
    "white_directory = os.path.join(directory, 'white_victims')\n",
    "os.makedirs(white_directory, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    file_path = os.path.join(directory, filename)\n",
    "    if file_path.startswith(white_directory):\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'race' in df.columns:\n",
    "        race_values = df['race'].astype(str)\n",
    "        if 'white' in race_values.unique():\n",
    "            dest_path = os.path.join(white_directory, filename)\n",
    "            shutil.move(file_path, dest_path)"
   ],
   "id": "8968345b248bb8b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5ae3605d307f7fab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
