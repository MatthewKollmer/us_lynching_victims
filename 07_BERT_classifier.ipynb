{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training and Implementing BERT to Label Clippings",
   "id": "43c691bd8c3718da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1) Overview\n",
    "\n",
    "Via the code and processes in 05_random_sample.ipynb and 06_label_training_data.ipynb, I created a training dataset (saved as [training_data.csv](https://raw.githubusercontent.com/MatthewKollmer/us_lynching_victims/refs/heads/main/training_data.csv)) and a test set (saved as [test_data.csv](https://raw.githubusercontent.com/MatthewKollmer/us_lynching_victims/refs/heads/main/test_data.csv)). Using these data sources, I explored several text classification methods (TF-IDF logistic regression, BoW logistic regression, and BERT-base). Ultimately, fine-tuning BERT yielded the best results, so I went ahead and classified the rest of our data with BERT. \n",
    "\n",
    "This notebook showcases our BERT classification tests and results. It also shows how I ran our fine-tuned BERT model on the rest of the data three times, creating multiple BERT classifications that I averaged for our final probability labels. All those steps are explained below. However, this notebook omits the numerous tests using logistic regression, tests with different versions of the training data, and tests across different values for BERT's parameters. While those less accurate attempts and explorations were important for us to land on our final methods, they're not necessary to understand the resultant data of classified US lynching victim reports. That's why I'm omitting them here, but I may revisit the whole process in other notebooks down the line."
   ],
   "id": "e3fe66beb007a7ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob"
   ],
   "id": "c05405d8d8eab655",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) Loading BERT and Data\n",
    "\n",
    "If you're trying to replicate our processes, please be sure to review the following parameters to ensure they're compatible with your device. In particular, you'll need to check your torch.device() compatibility. I'm working on a M-series Macbook Pro, so my version of the device is 'mps' (meaning 'metal performance shaders'). If you're working on a PC with NVIDIA GPUs, you should try 'cuda'. If you're on an older device with only CPUs available, then you should try 'cpu'. Be warned, though: this code will take an unreasonable amount of time without the proper hardware and GPUs."
   ],
   "id": "c1d7c25d01cc9505"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load data, BERT, and tokenizer\n",
    "train_df = pd.read_csv('https://raw.githubusercontent.com/MatthewKollmer/us_lynching_victims/refs/heads/main/training_data.csv')\n",
    "test_df = pd.read_csv('test_data/test_data.csv')\n",
    "# device here basically tells your computer what part of the hardware should handle running BERT. GPUs are necessary.\n",
    "device = torch.device('mps')\n",
    "# I'm using bert-base-uncased. There are other models and BERT variations available here, but BERT base is well documented and capable for our task.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)"
   ],
   "id": "fe99026464ec445b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3) Transforming Data for BERT\n",
    "\n",
    "In order to use BERT for classification, it's necessary to convert textual data into numbers. This is just how the model is designed to read the data, so we convert 'no' labels to 0 and 'yes' labels to 1, then convert them back for easier human reading after BERT completes its classifications."
   ],
   "id": "276bd47c0ac2e6cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerize_labels = {'no': 0, 'yes': 1}\n",
    "textualize_labels = {0: 'no', 1: 'yes'}\n",
    "train_labels_numeric = train_df['case_match'].map(numerize_labels).values\n",
    "test_labels = test_df['case_match'].map(numerize_labels).values"
   ],
   "id": "81fdb72259f91131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BERT also runs on data structures called tensors. In a nutshell, these are high dimensional data structures that give BERT the capacity to process textual data. The following function tokenize_clippings() essentially takes the clippings in our data, tokenizes them, and transforms the tokens into tensor data.",
   "id": "cbcc5352bffb39d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This is a tricky part. BERT needs tensor data to make its predictions. This function turns the strings in 'clippings' into tensor data for our training purposes.\n",
    "def tokenize_clippings(clippings, labels, max_length=155): \n",
    "    # max_length at 155 corresponds to the word length of our clippings\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    token_type_ids_list = []\n",
    "\n",
    "    for clip in clippings:\n",
    "        encoding = tokenizer.encode_plus(clip, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length',return_tensors='pt', return_token_type_ids=True, return_attention_mask=True)\n",
    "        input_ids_list.append(encoding['input_ids'].flatten())\n",
    "        attention_mask_list.append(encoding['attention_mask'].flatten())\n",
    "        token_type_ids_list.append(encoding['token_type_ids'].flatten())\n",
    "\n",
    "    input_ids = torch.stack(input_ids_list)\n",
    "    attention_masks = torch.stack(attention_mask_list)\n",
    "    token_type_ids = torch.stack(token_type_ids_list)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, token_type_ids, labels_tensor)"
   ],
   "id": "6cc18a353421193",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With tokenize_clippings(), we create tensors for BERT:",
   "id": "2b01ab8e4b8e9302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = tokenize_clippings(train_df['clippings'].values, train_labels_numeric, max_length=155)\n",
    "test_dataset = tokenize_clippings(test_df['clippings'].values, test_labels, max_length=155)"
   ],
   "id": "9cefd60c8503040e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using DataLoader, I'm feeding our training and test data to BERT in batches. This is just so it doesn't try to process the entire dataset at once (which would be an incredible strain on my computer).",
   "id": "8029cd0e91e65097"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A loader so I don't fry my pooter\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ],
   "id": "8095860a3943171a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I also set the model optimizer and learning rate. After numerous tests, I found a learning rate of 4e-5 does best for recall. There is slight degradation to precision, but I address that in other steps below.",
   "id": "ed62fe7097aafc70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "optimizer = AdamW(model.parameters(), lr=4e-5)",
   "id": "d10749ac048e049e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Fine-Tuning BERT for Our Classification Task\n",
    "\n",
    "With the data prepared for BERT, I'm now ready to fine-tune the model for our classification task. The following loop does so with progress bars to monitor the process and the average loss. Just a heads up: this step can take some time, depending on your device. On my M3 Max Macbook Pro, it takes about 8 minutes. On older devices with less GPUs, it will take much longer.\n",
    "\n",
    "One thing I've monitored is the average loss between training epochs. In this context, loss is basically a measure of the model's accuracy in predicting tokens in the training data. If the loss decreases between epochs, it means the model is learning something about the training data. Each time I ran the model, the first epoch saw an average loss of roughly .23. By the fifth epoch, the average loss always dropped to roughly .05."
   ],
   "id": "a65eb35165be2d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# commence training\n",
    "epochs = 5 # I've tried 3, 4, 5, and 6. The more epochs, the better the model does (up to five where it plateaus)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_mask, token_type_ids, labels = [t.to(device) for t in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'epoch {epoch+1} - avg loss: {avg_loss:.4f}')"
   ],
   "id": "b8573f47339aa9d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5) Evaluating BERT\n",
    "\n",
    "Before running our BERT classifier on the entirety of our data, I tested it on our test set."
   ],
   "id": "8bf26720438e7dec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# let's see how it does on our test data\n",
    "model.eval()\n",
    "predictions = []\n",
    "all_labels = []\n",
    "progress_bar = tqdm(test_loader, desc='Evaluation', unit='batch')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_mask, token_type_ids, labels = [t.to(device) for t in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())"
   ],
   "id": "cd209431be558201",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here's how I reported the results:",
   "id": "32e1c9e04f158e77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predicted_labels = [textualize_labels[p] for p in predictions]\n",
    "test_df['predicted_case_match'] = predicted_labels\n",
    "print(classification_report(test_df['case_match'], test_df['predicted_case_match']))"
   ],
   "id": "e488c63f44508346",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Across each iteration of fine-tuning BERT, it reached an overall accuracy of 93-95%. I found that lower learning rates (2e-5 in particular) yielded higher overall accuracy by increasing precision scores but lowered recall for 'yes' labels. While it may seem counterintuitive, I chose to implement higher learning rates despite lower precision because a higher learning rate consistently resulted in higher recall (above 90%). Recall is more important to us, because I want to identify as many references to lynchings as possible. Since I was able to fine-tune the model three separate times and maintain the results each time, I also have a method for combining BERT results into final classifications (described below).\n",
    "\n",
    "Results with learning rate at 4e-5, max_length 155, and epochs 5:\n",
    "\n",
    "FIRST IMPLEMENTATION (BERT_1):\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          no       0.99      0.93      0.96       161\n",
    "         yes       0.77      0.95      0.85        39\n",
    "\n",
    "    accuracy                           0.94       200\n",
    "\n",
    "SECOND IMPLEMENTATION (BERT_2):\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          no       0.99      0.92      0.95       161\n",
    "         yes       0.74      0.95      0.83        39\n",
    "\n",
    "    accuracy                           0.93       200\n",
    "    \n",
    "THIRD IMPLEMENTATION (BERT_3):\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          no       1.00      0.94      0.97       161\n",
    "         yes       0.81      1.00      0.90        39\n",
    "\n",
    "    accuracy                           0.95       200\n"
   ],
   "id": "2dfe6e559e3ad71e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6) Classifying Aggregate Data\n",
    "\n",
    "After each fine-tuning version of BERT, I ran the model on the aggregate data and saved its classifications as separate columns (BERT_1, BERT_2, and BERT_3). The following code demonstrates how to task the fine-tuned model. \n",
    "\n",
    "- It starts by creating a progress bar. \n",
    "- It includes a loop to check if there are already labels in the given column (in case you have to pause and come back, since this code takes a while). \n",
    "- It creates a mask of just the rows where the 'clippings' column contains text. This is so the model doesn't try to guess at blank rows. It labels any blank rows as 'no'.\n",
    "- It prepares the clippings in each csv for BERT.\n",
    "- It saves the BERT predictions in a new column."
   ],
   "id": "39d7a2adf4516b21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# count the total rows across csvs so I can keep track of progress\n",
    "directory = 'name_clusters'\n",
    "csv_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "total_rows = 0\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    total_rows += len(df)"
   ],
   "id": "e44b66bbf807bb22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# I ran this code three times, changing the 'BERT_{}' to the corresponding runs, thereby creating three columns with separate BERT classifications (BERT_1, BERT_2, BERT_3)\n",
    "progress_bar = tqdm(total=total_rows, desc='Rows', unit='rows')\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # check BERT iteration here\n",
    "    if 'BERT_3' in df.columns:\n",
    "        progress_bar.update(len(df))\n",
    "        continue\n",
    "        \n",
    "    df['clippings'] = df['clippings'].astype(str)\n",
    "    mask_valid = (df['clippings'].notnull() & (df['clippings'].str.strip() != '') & (df['clippings'] != 'NaN'))\n",
    "    df_valid = df[mask_valid].copy()\n",
    "    df_invalid = df[~mask_valid].copy()\n",
    "    # check BERT iteration here\n",
    "    df_invalid['BERT_3'] = 'no'\n",
    "\n",
    "    if df_valid.empty:\n",
    "        progress_bar.update(len(df))\n",
    "        df_merged = pd.concat([df_valid, df_invalid], axis=0).sort_index()\n",
    "        df_merged.to_csv(csv_file, index=False)\n",
    "        continue\n",
    "\n",
    "    dummy_labels = [0] * len(df_valid)\n",
    "    clippings_dataset = tokenize_clippings(df_valid['clippings'].values, dummy_labels, max_length=155)\n",
    "    clippings_loader = DataLoader(clippings_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in clippings_loader:\n",
    "            input_ids, attention_mask, token_type_ids, _ = [t.to(device) for t in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            progress_bar.update(len(input_ids))\n",
    "\n",
    "    predicted_labels = [textualize_labels[p] for p in predictions]\n",
    "    # check BERT iteration here\n",
    "    df_valid['BERT_3'] = predicted_labels\n",
    "\n",
    "    df_merged = pd.concat([df_valid, df_invalid], axis=0).sort_index()\n",
    "    df_merged.to_csv(csv_file, index=False)\n",
    "\n",
    "progress_bar.close()"
   ],
   "id": "8267306207d64499",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7) Averaging Predictions for Final Results\n",
    "\n",
    "After running BERT each time, I assessed the results. First I calculated how many clippings BERT classified as yes (that is, 'yes, the clipping contains reference to lynching')."
   ],
   "id": "dd90a30ecf9d9709"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_yes = 0\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file, usecols=['BERT_3'])\n",
    "    file_yes_count = (df['BERT_3'] == 'yes').sum()\n",
    "    total_yes += file_yes_count\n",
    "\n",
    "print(f'Total yes count: {total_yes}')"
   ],
   "id": "b983509da2a62de3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Each Run: \n",
    "\n",
    "- BERT_1 has 76,653 clippings labelled as containing references to lynchings (about 17% of the data)\n",
    "\n",
    "- BERT_2 has 81,256 clippings labelled as containing references to lynchings (about 18% of the data)\n",
    "\n",
    "- BERT_3 has 77,297 clippings labelled as containing references to lynchings (about 17% of the data)\n",
    "\n",
    "Then I calculated the average correspondence (how often all three iterations of BERT made the same predictions)."
   ],
   "id": "744a617117f91892"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "overall_matches = 0\n",
    "overall_rows = 0\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file, usecols=['BERT_1', 'BERT_2', 'BERT_3'])\n",
    "    total = len(df)\n",
    "    matches = ((df['BERT_1'] == df['BERT_2']) & (df['BERT_2'] == df['BERT_3'])).sum()\n",
    "    overall_matches += matches\n",
    "    overall_rows += total\n",
    "    average_correspondence = overall_matches / overall_rows\n",
    "    \n",
    "print(f'Average correspondence: {average_correspondence:.2%}')\n",
    "# Average correspondence: 94.82%"
   ],
   "id": "20d40297e7acf3d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The average correspondence was 94.82%\n",
    "\n",
    "Finally, I created a final probability label using the three BERT predictions. In the function below, I label every empty 'clippings' row with a 'probability' unknown. If each iteration of BERT classified the clipping as 'yes', I labelled its probability as 'high'. If two BERT iterations labelled it 'yes', the probability is 'medium'. If only one 'BERT' iteration labelled the clipping 'yes', I set the probability to 'low'. If no BERT iterations labelled the clipping 'yes', the probability label is 'unlikely'. "
   ],
   "id": "16b2743953eed588"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_probability(row):\n",
    "    if row[['clippings']].isnull().any() or row['clippings'] == 'NaN':\n",
    "        return 'unknown'\n",
    "    \n",
    "    yes_count = sum(row[col] == 'yes' for col in ['BERT_1','BERT_2','BERT_3']) \n",
    "    if yes_count == 3:\n",
    "        return 'high'\n",
    "    elif yes_count == 2:\n",
    "        return 'medium'\n",
    "    elif yes_count == 1:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'unlikely'"
   ],
   "id": "ad41a47b71e679a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['probability'] = df.apply(calculate_probability, axis=1)\n",
    "    df.to_csv(csv_file, index=False)"
   ],
   "id": "c5779aaa841ae8a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, to review the numbers of each label, I counted their totals and printed below:",
   "id": "dfa232895a2adf46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "overall_counts = pd.Series(dtype=int)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file, usecols=['probability'])    \n",
    "    file_counts = df['probability'].value_counts()\n",
    "    overall_counts = overall_counts.add(file_counts, fill_value=0)\n",
    "\n",
    "overall_counts = overall_counts.sort_values(ascending=False)\n",
    "print(overall_counts)"
   ],
   "id": "3de55b96d5ef7ae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Counts per probability label:\n",
    "\n",
    "- unlikely:    273,440\n",
    "- unknown:      88,462\n",
    "- high:         67,482\n",
    "- low:          14,210\n",
    "- medium:        9,257"
   ],
   "id": "99f9d078c2a95638"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
