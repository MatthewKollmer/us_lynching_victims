{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparing Data for the Digital Exhibit",
   "id": "fed6033a48f3f41d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "from lxml import etree"
   ],
   "id": "7228016772b9b062",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1) Overview\n",
    "\n",
    "The following notebook presents my steps in code for preparing our data for the digital exhibit. These steps include narrowing the data by 'high' probability labels, making sure the data includes the necessary entries for columns crucial for the exhibit, and pulling more data from Chronicling America's API. There were some external steps not present hereâ€“mainly, those of building a prototype of the exhibit first and then testing it on samples of our data. But suffice it to say that I first built the prototype and gleaned what aspects of the data it would need to function. Then I constructed the following pipeline to make those aspects available across as much of our data as possible.\n",
    "\n",
    "Also, you may notice me dropping data throughout the pipeline below. This is not a disastrous oversight. Prior to running this code, I made a copy of all our data, a copy I could prepare specifically for the exhibit. This means we have a \"core\" dataset from our previous processes and an \"exhibit\" dataset which is essentially a reconstituted subset of the core data. After running all the code below, the exhibit data contains 31,257 clippings out of what was initially 51,901 'high' probability lynching reports. It also represents 2,241 Black victims out of what was initially 3,084."
   ],
   "id": "64bcfc3334b75a54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2) Preliminary Directories and Functions",
   "id": "fe7f2907724371e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# directory paths used throughout this notebook\n",
    "directory = 'exhibit_prep/black_victims'\n",
    "csv_paths = sorted(glob(os.path.join(directory, '*.csv')))\n",
    "\n",
    "# function to count rows quickly. Used frequently below.\n",
    "def fast_row_count(path: str) -> int:\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as csv_file:\n",
    "            rows = sum(1 for _ in csv_file)\n",
    "        return max(0, rows - 1)\n",
    "    except Exception:\n",
    "        return len(pd.read_csv(path))\n",
    "    \n",
    "# scrape_carefully() function adapted again, used at two separate points below\n",
    "def scrape_carefully(url, retries=3, timeout=30): # 30 second timeout b/c new Chron Am API is SLOOOOOOW - must skip pages that take longer than 30 seconds\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                time.sleep(3)  # set to 3 because that respects LoC's limit of 20 requests per minute. For more on LoC rate limits, visit https://www.loc.gov/apis/json-and-yaml/working-within-limits/\n",
    "                return response\n",
    "\n",
    "            if response.status_code == 429:\n",
    "                time.sleep(3600) # one hour, a long wait time, but according to LoC, the time you will be banned if you get a 429 error\n",
    "                continue\n",
    "\n",
    "            # Other non-200 just sleep 3 seconds and move on\n",
    "            time.sleep(3)\n",
    "\n",
    "        except requests.RequestException:\n",
    "            time.sleep(3)\n",
    "\n",
    "    # after three retries, just move on\n",
    "    return None"
   ],
   "id": "29477781be5f230f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3) Update the urls to the new Chron Am format\n",
    "\n",
    "Because Chronicling America updated to a new API and website, I needed to update all the 'url' values in our data. This was fairly simple: I just used regex to match variables in the old URLs and then put those variables in the new URL structure."
   ],
   "id": "88bd70f2d05eeda3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def update_chron_am_urls(url):\n",
    "    match = re.search(r'lccn/(sn\\d+)/(\\d{4}-\\d{2}-\\d{2})/(ed-\\d+)/seq-(\\d+)', url)\n",
    "    sn_code, date, ed, seq = match.groups()\n",
    "    return f'https://www.loc.gov/resource/{sn_code}/{date}/{ed}/?sp={seq}'\n",
    "\n",
    "for file in tqdm(csv_paths, desc='Updating URLs'):\n",
    "    df = pd.read_csv(file)\n",
    "    if 'url' in df.columns:\n",
    "        df['url'] = df['url'].apply(update_chron_am_urls)\n",
    "        df.to_csv(file, index=False)"
   ],
   "id": "3e7b63547c0abfe4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4) Drop rows that are not labelled 'high' probability\n",
    "\n",
    "Since the exhibit is primarily concerned with presenting verified lynching reports, I subset the exhibit data by rows with 'high' probability labels. I also dropped any cases that didn't have any 'high' probability clippings."
   ],
   "id": "fb545bb5b615973f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_rows = sum(fast_row_count(path) for path in csv_paths)\n",
    "\n",
    "with tqdm(total=total_rows, desc='Filtering rows', unit='row') as pbar:\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        pbar.update(len(df))\n",
    "        \n",
    "        # only keep rows with 'high' probability\n",
    "        if 'probability' in df.columns:\n",
    "            df = df[df['probability'] == 'high'].copy()\n",
    "        \n",
    "        # delete csvs with no 'high' probability clippings\n",
    "        if len(df) == 0:\n",
    "            os.remove(path)\n",
    "        else:\n",
    "            df.to_csv(path, index=False)"
   ],
   "id": "5a87bd7ba7a41e0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5) Builds the manifest json url\n",
    "\n",
    "After a lot of investigation into the new Chronicling America API, I learned that I would need to call the JSON manifests for newspaper issues in order to get data to construct clipping image URLs for the exhibit. So, just as above, I built the manifest JSON URLs by matching variables in the page URL using regex. Then I put those variables into the manifest JSON URL format."
   ],
   "id": "abd248a8e48dffe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_manifest_url(url):\n",
    "    sn_code, date, ed = re.search(r'/resource/(sn\\d+)/(\\d{4}-\\d{2}-\\d{2})/(ed-\\d+)', url).groups()\n",
    "    return f'https://www.loc.gov/item/{sn_code}/{date}/{ed}/manifest.json'\n",
    "\n",
    "total_rows = sum(fast_row_count(path) for path in csv_paths)\n",
    "\n",
    "with tqdm(total=total_rows, desc='Building manifest URLs', unit='row') as pbar:\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        df['manifest'] = df['url'].apply(build_manifest_url)\n",
    "        pbar.update(len(df))\n",
    "        df.to_csv(path, index=False)"
   ],
   "id": "3ffe51e6c7f222ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6) scrapes the manifest json for the xml url\n",
    "\n",
    "After a lot of investigation into the new Chronicling America API, I also learned that I would need to pull the XML version of pages. The XML URLs are listed on the Manifest JSON. They contain word coordinates (i.e., the pixel location for each word as it appears on the page image). Within their URL structure, the XML URLs also have the batch file data and unique image file codes. These were yet more data I would need to construct the clipping image URLs.\n",
    "\n",
    "To gather these XML URLs, I called the Manifest JSONs from the API. Then I matched the XML URLs with the general page URLs from our 'url' columns. When there was a match, I pulled the xml_url.\n",
    "\n",
    "Also, just FYI: this code took days to run. The new API is slower and its rate limits are stricter, so if you're using the same process, please know it will take a long time for large amounts of data."
   ],
   "id": "4c9e4eb18cec552c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# function for navigating and pulling xml urls from LoC's manifest json files\n",
    "def pull_xml_url(manifest: dict, base_url: str):\n",
    "    canvases = None\n",
    "    \n",
    "    # accounts for both iiif versions (v3 and v2) just in case\n",
    "    if isinstance(manifest.get('items'), list):\n",
    "        canvases = manifest['items']\n",
    "    elif isinstance(manifest.get('sequences'), list) and manifest['sequences']:\n",
    "        canvases = manifest['sequences'][0].get('canvases', [])\n",
    "\n",
    "    if not isinstance(canvases, list):\n",
    "        return None\n",
    "    \n",
    "    # if 'related': 'base_url', pulls the next 'seeAlso': which is the corresponding xml_url\n",
    "    for canvas in canvases:\n",
    "        if canvas.get('related') == base_url:\n",
    "            xml_url = canvas.get('seeAlso')\n",
    "            return xml_url if isinstance(xml_url, str) else None\n",
    "        \n",
    "    return None\n",
    "\n",
    "total_rows = sum(fast_row_count(path) for path in csv_paths)\n",
    "\n",
    "with tqdm(total=total_rows, desc='xml_url progress', unit='row') as pbar:\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        # adds the necessary column to hold our xml urls\n",
    "        if 'xml_url' not in df.columns:\n",
    "            df['xml_url'] = ''\n",
    "        \n",
    "        # skip rows you've already done\n",
    "        for row in df.index:\n",
    "            if str(df.at[row, 'xml_url']).strip():\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            manifest_url = df.at[row, 'manifest']\n",
    "            base_url = df.at[row, 'url']\n",
    "            \n",
    "            # pulls and puts into xml_url column\n",
    "            response = scrape_carefully(manifest_url)\n",
    "            if response and response.status_code == 200:\n",
    "                manifest = json.loads(response.text)\n",
    "                xml = pull_xml_url(manifest, base_url)\n",
    "                df.at[row, 'xml_url'] = xml\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        df.to_csv(path, index=False)"
   ],
   "id": "1b4bba6b21fdbcdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7) Pull line coordinate data from the xml_url pages\n",
    "\n",
    "With the xml_urls, I was then able to pull the full text and its corresponding word coordinates for every page. I also pulled the 'HEIGHT' and 'WIDTH' values for every page. These refer to the number of pixels on each page from top to bottom, side to side.\n",
    "\n",
    "Again, this required API requests for thousands of rows. It took several days. If you're recreating these steps, be prepared for long runtimes."
   ],
   "id": "b6e674cfae5f526c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pull_xml_data(xml_url):\n",
    "    # load lxml parser to more easily navigate xml pages. See documentation here: https://lxml.de/\n",
    "    try:\n",
    "        parser = etree.XMLParser(resolve_entities=False, recover=True)\n",
    "        xml_file = etree.fromstring(xml_url, parser=parser)\n",
    "    except Exception:\n",
    "        return None, None, []\n",
    "\n",
    "    # pull pg_height and pg_width from top of xml pages\n",
    "    page_content = xml_file.xpath('//*[local-name()=\"Layout\"]/*[local-name()=\"Page\"]')\n",
    "    if not page_content:\n",
    "        page_content = xml_file.xpath('//*[local-name()=\"Page\"]')\n",
    "    pg_height = None\n",
    "    pg_width = None\n",
    "    if page_content:\n",
    "        page = page_content[0]\n",
    "        pg_height = page.get('HEIGHT')\n",
    "        pg_width = page.get('WIDTH')\n",
    "\n",
    "    # pulls the textline data, including line_id, WIDTH (width of line), HPOS, and VPOS, as well as the CONTENT values of each nested String ID\n",
    "    # feeds that data back as lists of dictionaries (very convenient)\n",
    "    text_line_data = []\n",
    "    lines = xml_file.xpath('//*[local-name()=\"TextLine\"]')\n",
    "    for line in lines:\n",
    "        line_id = line.get('ID') or ''\n",
    "        width = line.get('WIDTH') or ''\n",
    "        hpos = line.get('HPOS') or ''\n",
    "        vpos = line.get('VPOS') or ''\n",
    "        strings = line.xpath('./*[local-name()=\"String\"]')\n",
    "        contents = []\n",
    "        for string in strings:\n",
    "            word = string.get('CONTENT')\n",
    "            if isinstance(word, str) and word.strip():\n",
    "                contents.append(word.strip())\n",
    "        content_joined = ' '.join(contents).strip()\n",
    "        text_line_data.append({line_id: {'WIDTH': width, 'HPOS': hpos, 'VPOS': vpos, 'content': content_joined}})\n",
    "\n",
    "    return pg_height, pg_width, text_line_data\n",
    "\n",
    "total_rows = sum(fast_row_count(path) for path in csv_paths)\n",
    "\n",
    "# off to the races â€“ gonna take a loooong time, just so you know (days)\n",
    "with tqdm(total=total_rows, desc='pulling coordinate data', unit='row') as pbar:\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if 'pg_height' not in df.columns: df['pg_height'] = ''\n",
    "        if 'pg_width'  not in df.columns: df['pg_width']  = ''\n",
    "        if 'xml_content' not in df.columns: df['xml_content'] = ''\n",
    "\n",
    "        # for to skip the rows you've already pulled - only skips if all data is pulled (pg_height, pg_width, and xml_content)\n",
    "        for row in df.index:\n",
    "            done_height = bool(str(df.at[row, 'pg_height']).strip())\n",
    "            done_width  = bool(str(df.at[row, 'pg_width']).strip())\n",
    "            done_xml_content = bool(isinstance(df.at[row, 'xml_content'], str) and df.at[row, 'xml_content'].strip())\n",
    "            if done_height and done_width and done_xml_content:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "                \n",
    "            # also skip rows where xml_url is missing\n",
    "            xml_url = df.at[row, 'xml_url'] if 'xml_url' in df.columns else ''\n",
    "            if not (isinstance(xml_url, str) and xml_url.strip()):\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            response = scrape_carefully(xml_url)\n",
    "            if response and response.status_code == 200:\n",
    "                pg_height, pg_width, xml_content = pull_xml_data(response.content)\n",
    "                if pg_height: df.at[row, 'pg_height'] = pg_height\n",
    "                if pg_width: df.at[row, 'pg_width']  = pg_width\n",
    "                if xml_content: df.at[row, 'xml_content'] = json.dumps(xml_content, ensure_ascii=False)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        df.to_csv(path, index=False)"
   ],
   "id": "8defb9a0444b2947",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8) Check Results\n",
    "\n",
    "After all these API calls, I wanted to assess how many pages I had successfully compiled. The following code is how I measured those success rates.\n",
    "\n",
    "After two iterations of API calls and other data filtering, I had these results:\n",
    "\n",
    "- total rows: 51901\n",
    "- rows processed: 51901\n",
    "- total progress: 100.00%\n",
    "- xml content extracted: 50699\n",
    "- nan rows: 1202\n",
    "- total failed rows: 1202\n",
    "- success rate: 97.68%"
   ],
   "id": "58f1c2e2545012d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_rows = sum(fast_row_count(path) for path in csv_paths)\n",
    "\n",
    "total_rows_pulled = 0\n",
    "xml_count = 0\n",
    "nan_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for path in csv_paths:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    if not {'pg_height', 'pg_width', 'xml_content'}.issubset(df.columns):\n",
    "        continue\n",
    "\n",
    "    total_rows_pulled += len(df)\n",
    "    mask_xml = df['xml_content'].notna() & (df['xml_content'] != '')\n",
    "    xml_count += mask_xml.sum()\n",
    "    nan_count += df['xml_content'].isna().sum()\n",
    "    failed_count += (~mask_xml).sum()\n",
    "\n",
    "print(f'total rows: {total_rows}')\n",
    "print(f'rows processed: {total_rows_pulled}')\n",
    "print(f'total progress: {total_rows_pulled/total_rows * 100:.2f}%')\n",
    "print(f'xml content extracted: {xml_count}')\n",
    "print(f'nan rows: {nan_count}')\n",
    "print(f'total failed rows: {failed_count}')\n",
    "print(f'success rate: {xml_count/total_rows_pulled * 100:.2f}%')\n",
    "\n",
    "# total rows: 51901\n",
    "# rows processed: 51901\n",
    "# total progress: 100.00%\n",
    "# xml content extracted: 50699\n",
    "# nan rows: 1202\n",
    "# total failed rows: 1202\n",
    "# success rate: 97.68%"
   ],
   "id": "fb84a62baedcf8bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9) Extract clippings from the xml_content\n",
    "\n",
    "From the xml_content, I then pulled estimated clippings of the text around the victim's name. I did this by first identifying the text line that contained the victim's name. Then I extracted the three lines before the name and twenty lines after the victim's name."
   ],
   "id": "238498143a256c3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# a lil' function for parsing the xml_content correctly, avoiding errors from NaN values or empty strings\n",
    "def load_xml_content(cell: str):\n",
    "    try:\n",
    "        data = json.loads(cell) if isinstance(cell, str) and cell.strip() else []\n",
    "        return data if isinstance(data, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# a lil' function for ensuring that victim names and xml_content are read as strings then lowercased (to make them comparable)\n",
    "def lower_case_prep(string: str) -> str:\n",
    "    return (string or '').lower()\n",
    "\n",
    "total_rows = sum(fast_row_count(path) for path in csv_paths)\n",
    "\n",
    "with tqdm(total=total_rows, desc='pulling xml_clippings', unit='row') as pbar:\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if 'xml_clippings' not in df.columns:\n",
    "            df['xml_clippings'] = ''\n",
    "        \n",
    "        # skips rows already done\n",
    "        for row in df.index:\n",
    "            if isinstance(df.at[row, 'xml_clippings'], str) and df.at[row, 'xml_clippings'].strip():\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            # load the necessary data into these objects\n",
    "            victim = df.at[row, 'victim']\n",
    "            xml_content = df.at[row, 'xml_content'] if 'xml_content' in df.columns else ''\n",
    "\n",
    "            if not (isinstance(victim, str) and victim.strip() and isinstance(xml_content, str) and xml_content.strip()):\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # just make sure they're strings and lowercased and read correctly using my previously defined functions\n",
    "            victim_name = lower_case_prep(victim)\n",
    "            xml_lines = load_xml_content(xml_content)\n",
    "\n",
    "            # builds a list of xml content to more easily search for victim's name\n",
    "            text_line_data = []\n",
    "            for line in xml_lines:\n",
    "                if not isinstance(line, dict) or not line:\n",
    "                    continue\n",
    "                line_id, line_data = next(iter(line.items()))\n",
    "                content = line_data.get('content', '') if isinstance(line_data, dict) else ''\n",
    "                text_line_data.append((len(text_line_data), line, lower_case_prep(content)))\n",
    "\n",
    "            # reviews each line of xml content and identifies where the victim's name appears\n",
    "            victim_name_index = None\n",
    "            for index, line, content in text_line_data:\n",
    "                if victim_name and victim_name in content:\n",
    "                    victim_name_index = index\n",
    "                    break\n",
    "            \n",
    "            # once name is identified in the xml, pulls:\n",
    "            if victim_name_index is not None:\n",
    "                # three lines of text before name (and other TextLine ID data)\n",
    "                start = max(0, victim_name_index - 3)\n",
    "                # 20 lines of text after name (and other TextLine ID data)\n",
    "                end = min(len(text_line_data) - 1, victim_name_index + 20)\n",
    "                subset = [text_line_data[j][1] for j in range(start, end + 1)]\n",
    "                # and puts those new xml clippings into a new column\n",
    "                df.at[row, 'xml_clippings'] = json.dumps(subset, ensure_ascii=False)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        df.to_csv(path, index=False)"
   ],
   "id": "b69e18b0fa050ed0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10) Check Results Again\n",
    "\n",
    "Then I checked the results again to see how much data I had prepared. I had a hunch that OCR errors in victim names would mean a fair number of missed clipping extractions. I think I was correct. The results were as follows:\n",
    "\n",
    "- total rows: 51901\n",
    "- rows processed: 51901\n",
    "- total progress: 100.00%\n",
    "- clippings extracted: 40425\n",
    "- nan rows: 11476\n",
    "- total failed rows: 11476\n",
    "- success rate: 77.89%"
   ],
   "id": "68ab496caa355dbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_rows_pulled = 0\n",
    "xml_count = 0\n",
    "nan_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for path in csv_paths:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    if 'xml_clippings' not in df.columns:\n",
    "        continue\n",
    "\n",
    "    total_rows_pulled += len(df)\n",
    "    mask_xml = df['xml_clippings'].notna() & (df['xml_clippings'] != '')\n",
    "    xml_count += mask_xml.sum()\n",
    "    nan_count += df['xml_clippings'].isna().sum()\n",
    "    failed_count += (~mask_xml).sum()\n",
    "\n",
    "print(f'total rows: {total_rows}')\n",
    "print(f'rows processed: {total_rows_pulled}')\n",
    "print(f'total progress: {total_rows_pulled/total_rows * 100:.2f}%')\n",
    "print(f'clippings extracted: {xml_count}')\n",
    "print(f'nan rows: {nan_count}')\n",
    "print(f'total failed rows: {failed_count}')\n",
    "print(f'success rate: {xml_count/total_rows_pulled * 100:.2f}%')\n",
    "\n",
    "# total rows: 51901\n",
    "# rows processed: 51901\n",
    "# total progress: 100.00%\n",
    "# clippings extracted: 40425\n",
    "# nan rows: 11476\n",
    "# total failed rows: 11476\n",
    "# success rate: 77.89%"
   ],
   "id": "b582de0283bda236",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 11) Build clipping_image_url\n",
    "\n",
    "At this point, I had everything I needed to build the clipping image urls. These urls follow this pattern:\n",
    "\n",
    "https://tile.loc.gov/image-services/iiif/service:ndnp:{submitter_code}:batch_{tarfile_name}:data:{sn_code}:{reel_no}:{date_edition_no}:{image_endpoint}/pct:{coordinate_x},{coordinate_y},{coordinate_w},coordinate_h}/!600,600/0/default.jpg\n",
    "\n",
    "That's a base url of \"https://tile.loc.gov/image-services/iiif/service:ndnp:\" followed by: \n",
    "- submitter_code: extracted from xml_url\n",
    "- tarfile_name: extracted from xml_url\n",
    "- sn_code: extracted from xml_url\n",
    "- reel_no: extracted from xml_url\n",
    "- date_edition_no: extracted from xml_url\n",
    "- image_endpoint: extracted from xml_url\n",
    "\n",
    "Then the coordinates. These coordinates were calculated as follows:\n",
    "\n",
    "- coordinate_x: lowest HPOS value in xml_clipping / pg_width\n",
    "- coordinate_y: VPOS of the first line in xml_clipping / pg_height\n",
    "- coordinate_w: highest WIDTH value in xml_clipping / pg_width\n",
    "- coordinate_h: VPOS of the last line in xml_clipping - VPOS of the first line / pg_height"
   ],
   "id": "667be90623ee4b24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# regex pattern for extracting all necessary data from the xml_url\n",
    "url_pattern = re.compile(r'/service/ndnp/([^/]+)/batch_([^/]+)/data/(sn\\d+)/([^/]+)/([^/]+)/([^/]+)\\.xml$')\n",
    "\n",
    "# function that takes necessary data from the xml_url\n",
    "def parse_xml_url(xml_url):\n",
    "    match = url_pattern.search(xml_url or '')\n",
    "    if not match:\n",
    "        return None\n",
    "    submitter_code, tarfile_name, sn_code, reel_no, date_edition_no, image_endpoint = match.groups()\n",
    "    return {'submitter_code': submitter_code, 'tarfile_name': tarfile_name, 'sn_code': sn_code, 'reel_no': reel_no, 'date_edition_no': date_edition_no, 'image_endpoint': image_endpoint}\n",
    "\n",
    "# function for turning ints into floats (need to keep things readable)\n",
    "def ensure_floats(number):\n",
    "    try:\n",
    "        if number is None:\n",
    "            return None\n",
    "        if isinstance(number, (int, float)):\n",
    "            return float(number)\n",
    "        string = str(number).strip().replace(',', '')\n",
    "        return float(string)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# little function for loading the json lists/dictionaries in xml_clipping column\n",
    "def load_clipping_list(cell: str):\n",
    "    try:\n",
    "        data = json.loads(cell) if isinstance(cell, str) and cell.strip() else []\n",
    "        return data if isinstance(data, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# meaty function for pulling and building clipping_image_url\n",
    "# it puts it all together from sources and calculates the coordinate values\n",
    "def build_image_url(xml_url, pg_width, pg_height, clip_list: list):\n",
    "    parts = parse_xml_url(xml_url)\n",
    "    if not parts:\n",
    "        return ''\n",
    "\n",
    "    pg_width = ensure_floats(pg_width)\n",
    "    pg_height = ensure_floats(pg_height)\n",
    "    if not pg_width or not pg_height:\n",
    "        return ''\n",
    "\n",
    "    # prep the xml_clippings\n",
    "    clipping_lists = []\n",
    "    for entry in clip_list:\n",
    "        if isinstance(entry, dict) and entry:\n",
    "            _, data = next(iter(entry.items()))\n",
    "            if isinstance(data, dict):\n",
    "                clipping_lists.append(data)\n",
    "    if not clipping_lists:\n",
    "        return ''\n",
    "\n",
    "    # assign the pertinent VPOS values - easy since they're just highest or lowest in the clipping lines\n",
    "    # plus some contingencies in case they're out of order or missing\n",
    "    vpos_first = ensure_floats(clipping_lists[0].get('VPOS'))\n",
    "    vpos_last  = ensure_floats(clipping_lists[-1].get('VPOS'))\n",
    "    if vpos_first is None or vpos_last is None:\n",
    "        return ''\n",
    "    if vpos_last < vpos_first:\n",
    "        vpos_first, vpos_last = vpos_last, vpos_first\n",
    "\n",
    "    # assign HPOS values - trickier since they must be combined with line WIDTH\n",
    "    hpos_values   = []\n",
    "    right_edges = []\n",
    "    for part in clipping_lists:\n",
    "        hpos = ensure_floats(part.get('HPOS'))\n",
    "        width  = ensure_floats(part.get('WIDTH'))\n",
    "        if hpos is not None:\n",
    "            hpos_values.append(hpos)\n",
    "            if width is not None:\n",
    "                right_edges.append(hpos + width)\n",
    "    if not hpos_values:\n",
    "        return ''\n",
    "    \n",
    "    min_hpos = min(hpos_values)\n",
    "    if right_edges:\n",
    "        max_right = max(right_edges)\n",
    "        width_span = max(0.0, max_right - min_hpos)\n",
    "    else:\n",
    "        width_max = max((ensure_floats(part.get('WIDTH')) for part in clipping_lists), default=None)\n",
    "        if width_max is None:\n",
    "            return ''\n",
    "        width_span = max(0.0, width_max)\n",
    "\n",
    "    height_span = max(0.0, vpos_last - vpos_first)\n",
    "    if width_span == 0.0 or height_span == 0.0:\n",
    "        return ''\n",
    "\n",
    "    # the math to calculate coordinate percentages\n",
    "    x_pct = round((min_hpos / pg_width) * 100.0, 1)\n",
    "    y_pct = round((vpos_first / pg_height) * 100.0, 1)\n",
    "    w_pct = round((width_span / pg_width) * 100.0, 1)\n",
    "    h_pct = round((height_span / pg_height) * 100.0, 1)\n",
    "\n",
    "    id_part = (f'service:ndnp:{parts[\"submitter_code\"]}:batch_{parts[\"tarfile_name\"]}:'f'data:{parts[\"sn_code\"]}:{parts[\"reel_no\"]}:{parts[\"date_edition_no\"]}:{parts[\"image_endpoint\"]}')\n",
    "    region = f'pct:{x_pct:.2f},{y_pct:.2f},{w_pct:.2f},{h_pct:.2f}'\n",
    "    return f'https://tile.loc.gov/image-services/iiif/{id_part}/{region}/!600,600/0/default.jpg'\n",
    "\n",
    "def first_numeric(df, row, *cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            v = df.at[row, c]\n",
    "            val = ensure_floats(v)\n",
    "            if val is not None:\n",
    "                return val\n",
    "    return None\n",
    "\n",
    "total_rows = sum(fast_row_count(path) for path in csv_paths)\n",
    "\n",
    "with tqdm(total=total_rows, desc='build clipping_image_url', unit='row') as pbar:\n",
    "    for path in csv_paths:\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if 'clipping_image_url' not in df.columns:\n",
    "            df['clipping_image_url'] = ''\n",
    "\n",
    "        for row in df.index:\n",
    "            if isinstance(df.at[row, 'clipping_image_url'], str) and df.at[row, 'clipping_image_url'].strip():\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            xml_url = df.at[row, 'xml_url'] if 'xml_url' in df.columns else ''\n",
    "            clips_raw = df.at[row, 'xml_clippings'] if 'xml_clippings' in df.columns else ''\n",
    "\n",
    "            if not (isinstance(xml_url, str) and xml_url.strip() and isinstance(clips_raw, str) and clips_raw.strip()):\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            pg_width = first_numeric(df, row, 'pg_width', 'WIDTH')\n",
    "            pg_height = first_numeric(df, row, 'pg_height', 'HEIGHT')\n",
    "            if pg_width is None or pg_height is None:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            clip_list = load_clipping_list(clips_raw)\n",
    "            url = build_image_url(xml_url, pg_width, pg_height, clip_list)\n",
    "            if url:\n",
    "                df.at[row, 'clipping_image_url'] = url\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        df.to_csv(path, index=False)"
   ],
   "id": "45b312cefed92be2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 12) Clean Files for Exhibit\n",
    "\n",
    "My exhibit data had everything I needed at this point, but it also had a lot of unnecessary columns and data that would cause issues with the functionality of the exhibit. I therefore decided to strip down the data to just the columns necessary for exhibit functionality and just the rows properly prepared for presentation. I also changed some column titles since they would appear as labels in the exhibit text."
   ],
   "id": "8822aaaac77886ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "directory = ('exhibit_prep/black_victims')\n",
    "csv_paths = sorted(glob(os.path.join(directory, '*.csv')))\n",
    "cols_to_drop = ['race', 'gender', 'sn_code', 'coverage', 'manifest', 'xml_url', 'pg_height', 'pg_width', 'xml_content', 'xml_clippings', 'probability', 'BERT_1', 'BERT_2', 'BERT_3', 'violence_word_count', 'racist_word_count', 'page_details']\n",
    "\n",
    "for path in csv_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna(subset=['clipping_image_url'])\n",
    "    df = df.dropna(subset=['lynch_latitude'])\n",
    "    df = df.dropna(subset=['lynch_longitude'])\n",
    "    pct_values = df['clipping_image_url'].str.extract(r'pct:([\\d.]+),([\\d.]+),([\\d.]+),([\\d.]+)')\n",
    "    pct_values[3] = pd.to_numeric(pct_values[3], errors='coerce')\n",
    "    mask = (pct_values[3].isna()) | (pct_values[3] <= 50.0)\n",
    "    df = df[mask]\n",
    "    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors= 'ignore')\n",
    "    df = df.rename(columns={'reprint_date': 'Article Date', 'reprint_longitude': 'article_longitude', 'reprint_latitude': 'article_latitude', 'newspaper': 'Newspaper'})\n",
    "    df['victim'] = df['victim'].astype(str).str.title()\n",
    "    df['lynch_location'] = df['lynch_location'].astype(str).str.title()\n",
    "    df['no_of_clippings'] = len(df)\n",
    "    \n",
    "    if df.empty:\n",
    "        os.remove(path)\n",
    "        continue\n",
    "        \n",
    "    df.to_csv(path, index=False)"
   ],
   "id": "a79c0e8f38ce2cd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13) Improve lynch_date presentation for the Exhibit\n",
    "\n",
    "I also wanted the lynch_date data to be properly structured to appear in the exhibit text. This was annoyingly tricky. There were a number of date formats in the lynch_date columns, so I did my best to account for them all. "
   ],
   "id": "1f28b1d7ee99f658"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# a function for parsing and adding suffixes to numbers\n",
    "def add_date_suffix(number: int) -> str:\n",
    "    if 11 <= number % 100 <= 13:\n",
    "        return f'{number}th'\n",
    "    return f\"{number}{ {1:'st', 2:'nd', 3:'rd'}.get(number % 10, 'th') }\"\n",
    "\n",
    "# a function for converting year into 4-digit version rather than 2-digit version\n",
    "# while accounting for our data range (1865-1921)\n",
    "# and doing best to parse dd/mm vs. mm/dd\n",
    "def format_month_date_year(text: str) -> str:\n",
    "    string = str(text).strip()\n",
    "    match = re.fullmatch(r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2})', string)\n",
    "    if not match:\n",
    "        return string\n",
    "    a, b, yy = map(int, match.groups())\n",
    "    yyyy = 1900 + yy if 0 <= yy <= 21 else 1800 + yy\n",
    "    \n",
    "    # if first number > 12, it's dd/mm/yyyy\n",
    "    if a > 12:\n",
    "        return f'{b}/{a}/{yyyy}'\n",
    "    else:\n",
    "        return f'{a}/{b}/{yyyy}'\n",
    "\n",
    "def normalize_lynch_dates(df: pd.DataFrame):\n",
    "    string = df['lynch_date'].astype(str).map(format_month_date_year)\n",
    "    dt = pd.to_datetime(string, errors='coerce', infer_datetime_format=True)\n",
    "    df['lynch_date'] = dt.apply(lambda x: f\"{x.strftime('%B')} {add_date_suffix(x.day)}, {x.year}\" if pd.notnull(x) else '')\n",
    "    return df\n",
    "\n",
    "for path in csv_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    df = normalize_lynch_dates(df)\n",
    "    df.to_csv(path, index=False)"
   ],
   "id": "31c73a014028a198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 14) Assess Final Amounts of Data for the Exhibit\n",
    "\n",
    "At this point, I wanted to check how much data I had remaining after all these processes. I checked by total number of newspaper clippings, which turned out to be 31,257 out of our initial 51,901. I also checked by total number of victim cases represented in the data, which turned out to be 2,241 victims out of our initial 3,084. So, depending on how you measure it, our exhibit presents about 60% or 72% of our total number of cases and lynching reports."
   ],
   "id": "c1fb135111566dbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fast_row_count(path: str) -> int:\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as csv_file:\n",
    "            rows = sum(1 for _ in csv_file)\n",
    "        return max(0, rows - 1)\n",
    "    except Exception:\n",
    "        return len(pd.read_csv(path))\n",
    "\n",
    "total_rows = sum(fast_row_count(path) for path in csv_paths)\n",
    "total_rows\n",
    "\n",
    "# 31,257 clippings (out of what was initially 51,901 lynching reports)"
   ],
   "id": "cda725cec4c53404",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "remaining_csvs = glob(os.path.join(directory, '*.csv'))\n",
    "\n",
    "print(f'Total CSVs remaining aka victims represented: {len(remaining_csvs)}')\n",
    "\n",
    "# Total CSVs remaining aka victims represented: 2,241 (out of what was initially 3,084)"
   ],
   "id": "b985d445fc6ab969",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 15) Build Maps for Exhibit\n",
    "\n",
    "As a final step, I also created the maps to be used for the exhibit."
   ],
   "id": "3f5770dab16772f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import folium\n",
    "\n",
    "directory = 'exhibit_prep/black_victims'\n",
    "saved_directory = 'clipping_maps'\n",
    "os.makedirs(saved_directory, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.csv'):\n",
    "        filepath = os.path.join(directory, file)\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            coord_columns = ['lynch_latitude', 'lynch_longitude', 'article_latitude', 'article_longitude']\n",
    "            df[coord_columns] = df[coord_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # start location is center of continental USA\n",
    "            map_center = [39.8283, -98.5795]\n",
    "            folium_map = folium.Map(location=map_center, zoom_start=4, tiles=\"CartoDB positron\")\n",
    "\n",
    "            # circle markers for lynching sites\n",
    "            for _, row in df.iterrows():\n",
    "                tooltip = (\n",
    "                    f\"Victim: {row['victim']}<br>\"\n",
    "                    f\"Date: {row['lynch_date']}<br>\"\n",
    "                    f\"Location: {row['lynch_location']}\"\n",
    "                )\n",
    "                folium.CircleMarker(\n",
    "                    location=[row['lynch_latitude'], row['lynch_longitude']],\n",
    "                    radius=10,\n",
    "                    color='darkred',\n",
    "                    fill=True,\n",
    "                    fill_color='darkred',\n",
    "                    fill_opacity=0.0,\n",
    "                    tooltip=tooltip\n",
    "                ).add_to(folium_map)\n",
    "\n",
    "            # circle markers for article printing locations\n",
    "            for _, row in df.dropna(subset=['article_latitude', 'article_longitude']).iterrows():\n",
    "                url = row.get('url', '')\n",
    "                url_link = f'<a href=\"{url}\" target=\"_blank\">Read the full page</a>' if pd.notnull(url) else ''\n",
    "                popup_html = (\n",
    "                    f\"<b>Newspaper:</b> {row['Newspaper']}<br>\"\n",
    "                    f\"<b>Article Date:</b> {row['Article Date']}<br>\"\n",
    "                    f\"{url_link}<br><br>\"\n",
    "                    f\"<b>Clipping:</b><br>{row['clippings']}\"\n",
    "                )\n",
    "\n",
    "                folium.CircleMarker(\n",
    "                    location=[row['article_latitude'], row['article_longitude']],\n",
    "                    radius=4,\n",
    "                    color='darkred',\n",
    "                    fill=True,\n",
    "                    fill_color='darkred',\n",
    "                    fill_opacity=0.8,\n",
    "                    popup=folium.Popup(popup_html, max_width=300)\n",
    "                ).add_to(folium_map)\n",
    "\n",
    "            output_path = os.path.join(saved_directory, file.replace('.csv', '.html'))\n",
    "            folium_map.save(output_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {file}: {e}')"
   ],
   "id": "e4264097fca05758",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
